<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - LAGraph code coverage report. Commit id: cc56ed4. Current time (UTC): 2024-08-30T17:14:30Z - experimental/algorithm/LAGraph_dnn.c</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">experimental/algorithm</a> - LAGraph_dnn.c<span style="font-size: 80%;"> (source / <a href="LAGraph_dnn.c.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">LAGraph code coverage report. Commit id: cc56ed4. Current time (UTC): 2024-08-30T17:14:30Z</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">16</td>
            <td class="headerCovTableEntry">16</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-08-30 17:16:41</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">1</td>
            <td class="headerCovTableEntry">1</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : //------------------------------------------------------------------------------</a>
<a name="2"><span class="lineNum">       2 </span>            : // LAGraph_dnn: sparse deep neural network</a>
<a name="3"><span class="lineNum">       3 </span>            : //------------------------------------------------------------------------------</a>
<a name="4"><span class="lineNum">       4 </span>            : </a>
<a name="5"><span class="lineNum">       5 </span>            : // LAGraph, (c) 2019-2022 by The LAGraph Contributors, All Rights Reserved.</a>
<a name="6"><span class="lineNum">       6 </span>            : // SPDX-License-Identifier: BSD-2-Clause</a>
<a name="7"><span class="lineNum">       7 </span>            : //</a>
<a name="8"><span class="lineNum">       8 </span>            : // For additional details (including references to third party source code and</a>
<a name="9"><span class="lineNum">       9 </span>            : // other files) see the LICENSE file or contact permission@sei.cmu.edu. See</a>
<a name="10"><span class="lineNum">      10 </span>            : // Contributors.txt for a full list of contributors. Created, in part, with</a>
<a name="11"><span class="lineNum">      11 </span>            : // funding and support from the U.S. Government (see Acknowledgments.txt file).</a>
<a name="12"><span class="lineNum">      12 </span>            : // DM22-0790</a>
<a name="13"><span class="lineNum">      13 </span>            : </a>
<a name="14"><span class="lineNum">      14 </span>            : // Contributed by Timothy A. Davis, Texas A&amp;M University</a>
<a name="15"><span class="lineNum">      15 </span>            : </a>
<a name="16"><span class="lineNum">      16 </span>            : //------------------------------------------------------------------------------</a>
<a name="17"><span class="lineNum">      17 </span>            : </a>
<a name="18"><span class="lineNum">      18 </span>            : // LAGraph_dnn: sparse deep neural network.</a>
<a name="19"><span class="lineNum">      19 </span>            : // Based on inferenceReLUvec.m by Jeremy Kepner, MIT.</a>
<a name="20"><span class="lineNum">      20 </span>            : </a>
<a name="21"><span class="lineNum">      21 </span>            : // Performs ReLU inference using input feature vectors Y0.</a>
<a name="22"><span class="lineNum">      22 </span>            : </a>
<a name="23"><span class="lineNum">      23 </span>            : // See http://graphchallenge.org/ for a description of the algorithm.</a>
<a name="24"><span class="lineNum">      24 </span>            : </a>
<a name="25"><span class="lineNum">      25 </span>            : // On input, Y0 is the initial feature vectors, of size nfeatures-by-nneurons.</a>
<a name="26"><span class="lineNum">      26 </span>            : // This format uses the graph convention that A(i,j) is the edge (i,j).</a>
<a name="27"><span class="lineNum">      27 </span>            : // Each row of Y0 is a single feature.</a>
<a name="28"><span class="lineNum">      28 </span>            : </a>
<a name="29"><span class="lineNum">      29 </span>            : // W is an array of size nlayers of sparse matrices.  Each W[layer] matrix has</a>
<a name="30"><span class="lineNum">      30 </span>            : // the same size: nneurons-by-nneurons.  W[layer] represents the DNN weights</a>
<a name="31"><span class="lineNum">      31 </span>            : // for that layer.</a>
<a name="32"><span class="lineNum">      32 </span>            : </a>
<a name="33"><span class="lineNum">      33 </span>            : // The Bias[layer] matrices are diagonal, and the same size as W[layer].</a>
<a name="34"><span class="lineNum">      34 </span>            : </a>
<a name="35"><span class="lineNum">      35 </span>            : // All matrices should have type GrB_FP32; the method will be very slow</a>
<a name="36"><span class="lineNum">      36 </span>            : // otherwise.</a>
<a name="37"><span class="lineNum">      37 </span>            : </a>
<a name="38"><span class="lineNum">      38 </span>            : // On output, Y is the computed result, of the same size as Y0.</a>
<a name="39"><span class="lineNum">      39 </span>            : </a>
<a name="40"><span class="lineNum">      40 </span>            : #define LG_FREE_ALL         \</a>
<a name="41"><span class="lineNum">      41 </span>            : {                           \</a>
<a name="42"><span class="lineNum">      42 </span>            :     GrB_free (&amp;Y) ;         \</a>
<a name="43"><span class="lineNum">      43 </span>            : }</a>
<a name="44"><span class="lineNum">      44 </span>            : </a>
<a name="45"><span class="lineNum">      45 </span>            : #include &quot;LG_internal.h&quot;</a>
<a name="46"><span class="lineNum">      46 </span>            : #include &quot;LAGraphX.h&quot;</a>
<a name="47"><span class="lineNum">      47 </span>            : </a>
<a name="48"><span class="lineNum">      48 </span>            : //****************************************************************************</a>
<a name="49"><span class="lineNum">      49 </span><span class="lineCov">          2 : GrB_Info LAGraph_dnn    // returns GrB_SUCCESS if successful</span></a>
<a name="50"><span class="lineNum">      50 </span>            : (</a>
<a name="51"><span class="lineNum">      51 </span>            :     // output</a>
<a name="52"><span class="lineNum">      52 </span>            :     GrB_Matrix *Yhandle,    // Y, created on output</a>
<a name="53"><span class="lineNum">      53 </span>            :     // input: not modified</a>
<a name="54"><span class="lineNum">      54 </span>            :     GrB_Matrix *W,      // W [0..nlayers-1], each nneurons-by-nneurons</a>
<a name="55"><span class="lineNum">      55 </span>            :     GrB_Matrix *Bias,   // Bias [0..nlayers-1], diagonal nneurons-by-nneurons</a>
<a name="56"><span class="lineNum">      56 </span>            :     int nlayers,        // # of layers</a>
<a name="57"><span class="lineNum">      57 </span>            :     GrB_Matrix Y0       // input features: nfeatures-by-nneurons</a>
<a name="58"><span class="lineNum">      58 </span>            : )</a>
<a name="59"><span class="lineNum">      59 </span>            : {</a>
<a name="60"><span class="lineNum">      60 </span>            :     GrB_Info info ;</a>
<a name="61"><span class="lineNum">      61 </span><span class="lineCov">          2 :     char *msg = NULL ;</span></a>
<a name="62"><span class="lineNum">      62 </span>            : </a>
<a name="63"><span class="lineNum">      63 </span>            :     //--------------------------------------------------------------------------</a>
<a name="64"><span class="lineNum">      64 </span>            :     // check inputs</a>
<a name="65"><span class="lineNum">      65 </span>            :     //--------------------------------------------------------------------------</a>
<a name="66"><span class="lineNum">      66 </span>            : </a>
<a name="67"><span class="lineNum">      67 </span><span class="lineCov">          2 :     if (Yhandle == NULL || W == NULL || Bias == NULL || Y0 == NULL)</span></a>
<a name="68"><span class="lineNum">      68 </span>            :     {</a>
<a name="69"><span class="lineNum">      69 </span><span class="lineCov">          1 :         return (GrB_NULL_POINTER) ;</span></a>
<a name="70"><span class="lineNum">      70 </span>            :     }</a>
<a name="71"><span class="lineNum">      71 </span>            : </a>
<a name="72"><span class="lineNum">      72 </span>            :     //--------------------------------------------------------------------------</a>
<a name="73"><span class="lineNum">      73 </span>            :     // create the output matrix Y</a>
<a name="74"><span class="lineNum">      74 </span>            :     //--------------------------------------------------------------------------</a>
<a name="75"><span class="lineNum">      75 </span>            : </a>
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">          1 :     GrB_Matrix Y = NULL ;</span></a>
<a name="77"><span class="lineNum">      77 </span><span class="lineCov">          1 :     (*Yhandle) = NULL ;</span></a>
<a name="78"><span class="lineNum">      78 </span>            :     GrB_Index nfeatures, nneurons ;</a>
<a name="79"><span class="lineNum">      79 </span><span class="lineCov">          1 :     GRB_TRY (GrB_Matrix_nrows (&amp;nfeatures, Y0)) ;</span></a>
<a name="80"><span class="lineNum">      80 </span><span class="lineCov">          1 :     GRB_TRY (GrB_Matrix_ncols (&amp;nneurons,  Y0)) ;</span></a>
<a name="81"><span class="lineNum">      81 </span><span class="lineCov">          1 :     GRB_TRY (GrB_Matrix_new (&amp;Y, GrB_FP32, nfeatures, nneurons)) ;</span></a>
<a name="82"><span class="lineNum">      82 </span>            : </a>
<a name="83"><span class="lineNum">      83 </span>            :     //--------------------------------------------------------------------------</a>
<a name="84"><span class="lineNum">      84 </span>            :     // propagate the features through the neuron layers</a>
<a name="85"><span class="lineNum">      85 </span>            :     //--------------------------------------------------------------------------</a>
<a name="86"><span class="lineNum">      86 </span>            : </a>
<a name="87"><span class="lineNum">      87 </span><span class="lineCov">         31 :     for (int layer = 0 ; layer &lt; nlayers ; layer++)</span></a>
<a name="88"><span class="lineNum">      88 </span>            :     {</a>
<a name="89"><span class="lineNum">      89 </span>            :         // Y = Y * W [layer], using the conventional PLUS_TIMES semiring</a>
<a name="90"><span class="lineNum">      90 </span><span class="lineCov">         30 :         GRB_TRY (GrB_mxm (Y, NULL, NULL, GrB_PLUS_TIMES_SEMIRING_FP32,</span></a>
<a name="91"><span class="lineNum">      91 </span>            :             ((layer == 0) ? Y0 : Y), W [layer], NULL)) ;</a>
<a name="92"><span class="lineNum">      92 </span>            : </a>
<a name="93"><span class="lineNum">      93 </span>            :         // Y = Y * Bias [layer], using the MIN_PLUS semiring.  This computes</a>
<a name="94"><span class="lineNum">      94 </span>            :         // Y(i,j) += Bias [layer] (j,j) for each entry Y(i,j).  It does not</a>
<a name="95"><span class="lineNum">      95 </span>            :         // introduce any new entries in Y.  The MIN monoid is not actually used</a>
<a name="96"><span class="lineNum">      96 </span>            :         // since Bias [layer] is a diagonal matrix.  The prior version used</a>
<a name="97"><span class="lineNum">      97 </span>            :         // a PLUS_PLUS semiring, which also works but is not a GrB built-in.</a>
<a name="98"><span class="lineNum">      98 </span><span class="lineCov">         30 :         GRB_TRY (GrB_mxm (Y, NULL, NULL, GrB_MIN_PLUS_SEMIRING_FP32, Y,</span></a>
<a name="99"><span class="lineNum">      99 </span>            :             Bias [layer], NULL)) ;</a>
<a name="100"><span class="lineNum">     100 </span>            : </a>
<a name="101"><span class="lineNum">     101 </span>            :         // delete entries from Y: keep only those entries greater than zero</a>
<a name="102"><span class="lineNum">     102 </span><span class="lineCov">         30 :         GRB_TRY (GrB_select (Y, NULL, NULL, GrB_VALUEGT_FP32, Y, (float) 0,</span></a>
<a name="103"><span class="lineNum">     103 </span>            :             NULL));</a>
<a name="104"><span class="lineNum">     104 </span>            : </a>
<a name="105"><span class="lineNum">     105 </span>            :         // threshold maximum values: Y = min (Y, 32)</a>
<a name="106"><span class="lineNum">     106 </span><span class="lineCov">         30 :         GRB_TRY (GrB_apply (Y, NULL, NULL, GrB_MIN_FP32, Y, (float) 32, NULL)) ;</span></a>
<a name="107"><span class="lineNum">     107 </span>            :     }</a>
<a name="108"><span class="lineNum">     108 </span>            : </a>
<a name="109"><span class="lineNum">     109 </span>            :     //--------------------------------------------------------------------------</a>
<a name="110"><span class="lineNum">     110 </span>            :     // return result</a>
<a name="111"><span class="lineNum">     111 </span>            :     //--------------------------------------------------------------------------</a>
<a name="112"><span class="lineNum">     112 </span>            : </a>
<a name="113"><span class="lineNum">     113 </span><span class="lineCov">          1 :     (*Yhandle) = Y ;</span></a>
<a name="114"><span class="lineNum">     114 </span><span class="lineCov">          1 :     return (GrB_SUCCESS) ;</span></a>
<a name="115"><span class="lineNum">     115 </span>            : }</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
