--------------------------------------------------------------------------------
Feb 20. Results on plat8153, using gcc 7.4 with v3.2.0: SET 1
--------------------------------------------------------------------------------

max 32 threads, first set of results ('black box'): my1_*_plat scripts:

        #PBS -l nodes=1:ppn=2:plat8153
        #PBS -l walltime=24:00:00
        cd $PBS_O_WORKDIR
        export OMP_NUM_THREADS=32
        export KMP_AFFINITY="verbose,explicit,proclist=[0-31]"
        export GOMP_CPU_AFFINITY="0-31"
        numactl --interleave=all ./do_gap

default method for 32 threads:

                kron        urand       twitter      web        road
                -----       -----       -------     ----        ----
BC GrB          37.090      48.332      15.779       8.185      62.277
BFS GrB          3.125       2.183       1.157       1.072       7.557
CC GrB           3.433       4.548       1.486       1.730       0.807
PR GrB          16.541      18.860      12.891       6.345       0.919
SSSP GrB        16.315      18.954       7.634      11.981      77.365
TC GrB        1205.674      49.830     308.537      47.803       0.321

PR: uses method 3f, above (pure GraphBLAS); but note x4 is slightly faster.
    x4 uses import/export (see SET2 below).
TC: uses the 'dot2' default method; note 'dot' is sometimes faster
    (see SET2 below).

In all cases, exactly 32 threads were given to GraphBLAS, via
OMP_NUM_THREADS=32.  This value is queried by GraphBLAS as nthreads_max =
omp_get_max_threads().  Internally, for each parallel region, GraphBLAS uses
an autotuning mechanism, with a "#pragma omp parallel for ...
num_threads(nthreads)" clause, where nthreads is selected as

    nthreads = min (nthreads_max, max (floor (w/chunk), 1))
    
and where chunk = 2^16 is a user-controllable parameter.  "chunk" was left at
its default of 2^16 for all results, both SET1 and SET2.  GraphBLAS does this
autotuning by default for all of its parallel regions, so these results are
presented above.

--------------------------------------------------------------------------------
Feb 20. Results on plat8153, using gcc 7.4 or icc 19.1, with v3.2.0: SET 2
--------------------------------------------------------------------------------

max 64 threads, 2nd 'best effort' results, from my2_*_plat scripts.
with numactl --interleave=all, all other defaults (affinity, # threads):

        #PBS -l nodes=1:ppn=2:plat8153
        #PBS -l walltime=24:00:00
        cd $PBS_O_WORKDIR
        export OMP_NUM_THREADS=64
        numactl --interleave=all ./do_gap

                kron        urand       twitter      web        road
                -----       -----       -------     ----        ----
BC GrB          29.287      41.483      14.169      8.185      54.181
BFS GrB          2.767       2.068       1.043      1.010       7.455
CC GrB           3.002       3.931       1.335      1.718       0.716
PR GrB          15.258      18.083      11.642      5.643       0.769
SSSP GrB        16.315      18.870       7.634     11.981      62.739
TC GrB         802.497      29.849     197.667     36.653       0.292

BC:  same as SET 1, but with 64 threads for kron, urand, web;
     32 threads for web, 8 threads for road.
     using gcc exclusively.

BFS: same as SET 1, but with 64 threads for kron, urand, twitter, web;
     16 threads for road
     (but results almost identical for all # threads for road)
     icc: kron, twitter, web.  gcc: urand, road.

CC:  same as SET 1, but with 64 threads for all matrices.
     using gcc exclusively.

PR:  using method x4 (import/export) and 64 threads for all matrices
     (method 3f, pure GraphBLAS nearly as fast, however).  For comparison,
     all results with 64 threads (with gcc):
                kron        urand       twitter      web        road
                -----       -----       -------     ----        ----
     3f,#64:    15.508      18.401      11.833      6.091       0.870
     x4,#64:    15.303      18.202      11.642      5.915       0.769

     maybe just report 3f since it's pure GraphBLAS and only slightly
     slower?  If I use PR:3f instead of PR:x4, with 64 threads, I get the
     following (using icc for kron, urand, web; gcc for twitter and road):

                kron        urand       twitter      web        road
                -----       -----       -------     ----        ----
     PR:3f #64: 15.426      18.257      11.833      5.790       0.855

     For SET2:
     PR:x4 is faster with icc for kron, urand, and web.
     gcc is used for twitter and road.

SSSP: same as SET1 (32 threads for each, except using 16 threads for
    road, and 64 for urand).
    TODO: SSSP for road is probably faster with even fewer threads.
    Testing in progress.

TC:  using 64 for all but road, and 'dot' method instead of the default
    'dot2' for urand, twitter, and web.  Road graph with 32 threads
    (same as SET1).  The primary difference is to use 64 threads
    for all but road. (results on gcc):
                    kron        urand       twitter      web        road
                    -----       -----       -------     ----        ----
    TC (dot) #64    837.070     30.222 *    212.248 *   36.653 *    0.355
    TC (dot2)#64    829.292 *   31.356      219.430     38.040      0.345 *

    TC with icc is faster for kronk, urand, twitter, road.  using gcc for web.

SuiteSparse:GraphBLAS has an autotuning mechanism for selecting the number of
threads to use in each parallel region of each internal kernel.  It uses fewer
threads than the max available to it, if the work to do is low.  This allows
GraphBLAS to exploit hyperthreading, and thus most of the 64-thread results are
better than the 32-thread results (SET1), for most of the methods and graphs.

This mechanism is typically making reasonable choices, except for the road
graph, and in particular for SSSP.  For SSSP on the road graph, the run time
with 64 threads (with hyperthreading on 32 cores) is exceedingly high, however
(about 600 seconds, or about 10x the time for 32 threads).  It seems that in
this particular case (just SSSP with the road graph), the autotuner is making a
very bad choice for the number of threads to use, but only when hyperthreading
is in use.

An OpenMP-based program can query the max number of threads available to it,
but it cannot query what cores those threads reside on, and whether or not
hyperthreading is active.  As a result, its autotuning rule (see above)
cannot be modified when hyperthreading is enabled.

Each parallel region could be tuned with another constant parameter to revise
this number, as in nthreads = ... floor(w/(alpha*chunk))... , where alpha is a
hard-wired constant that depends on the particular parallel region that
reflects the relative performance of each parallel region, given an amount w of
work to do.  Clearly, from the SSSP results on the road graph, alpha needs to
be added to the appropriate internal parallel regions in GraphBLAS, so that
this excessive run time does not occur for the SSSP problem on the road graph
when hyperthreading is enabled.

